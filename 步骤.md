### 1. 数学与理论层（知道“为什么”）
| 知识点 | 必须掌握到什么程度 |
|---|---|
| 卷积/反卷积/空洞卷积 | 能推导输出尺寸公式，知道不同 dilation 对感受野的影响 |
| 损失函数 | 交叉熵、加权 CE、Focal Loss、Dice Loss、Combo Loss 的梯度推导，知道何时用谁 |
| 评价指标 | mIoU、FWIoU、Dice、Boundary IoU 的代码实现，能解释“边界”指标为什么更严格 |
| 感受野计算 | 能手写脚本算 Receptive Field，解释“大感受野≠好结果” |
| 多尺度融合 | ASPP、PPM、FPN、HRNet 融合公式的异同 |

---

### 2. 数据层（知道“喂什么”）
| 任务 | 动手目标 |
|---|---|
| 标注格式 | 能把任意数据集转成 COCO、Cityscapes、Mask 三种格式，写脚本互转 |
| 数据增强 | 实现“几何+颜色+纹理”组合：随机旋转、弹性形变、HSV、CLAHE、CutMix、Copy-Paste |
| 类别不平衡 | 会算权重、会画直方图、会在线重采样，能把 mIoU 提 3 个点以上 |
| 边缘优化 | 用 Canny/距离变换生成边界权重图，加权 CE 让边缘误差降 20% |

---

### 3. 模型层（知道“改哪里”）
| 任务 | 动手目标 |
|---|---|
| 复现经典 | 用 PyTorch 手写 U-Net、DeepLabv3+、BiSeNetV2，训练 Cityscapes 复现官方 mIoU 差距 ≤ 1% |
| 模块替换 | 把 ASPP 换成 DCM、把 FPN 换成 HRNet 融合块，跑消融实验写表格 |
| 轻量设计 | 把 DeepLabv3+ 参数量压到 < 2 M，mIoU 降 ≤ 3 个点，推理速度提升 ≥ 3× |
| Transformer 迁移 | Swin-T backbone 接任意解码器，用 Mask2Former 训练脚本跑通自定义数据 |

---

### 4. 训练与调参层（知道“怎么调”）
| 任务 | 动手目标 |
|---|---|
| 学习率策略 | 自己实现 Poly、Cosine、One-Cycle，对比“冻 backbone”与“全调”差异 |
| 损失平衡 | 多损失加权网格搜索 + 不确定性权重，把边界损失降到 0.5× 以下 |
| 显存优化 | 用 Gradient Checkpoint + Mixed Precision 把 batch 从 4 提到 16，速度不掉 |
| 错误分析 | 用混淆矩阵 + 边界 IoU 找出“道路-人行道”误分，针对性加 Copy-Paste 数据，提 2 个点 |

---

### 5. 部署与落地层（知道“怎么上线”）
| 任务 | 动手目标 |
|---|---|
| 推理加速 | 把 PyTorch → ONNX → TensorRT，1080p 输入 FPS 提升 ≥ 5×，精度掉 ≤ 0.5 mIoU |
| 边缘量化 | INT8 量化后模型大小 < 10 MB，树莓派 4 跑 30 fps，精度掉 ≤ 1 mIoU |
| 多任务统一 | 用 Mask2Former 一次输出语义+实例+全景，TensorRT 并行分支，延迟 < 150 ms |
| 在线更新 | 设计小样本增量训练流程：新增 100 张标注，30 分钟完成微调，mIoU 不掉 |

---

### 一张“通关路线图”
1. 先把 U-Net 在 CamVid 上复现到 70 mIoU（一周）  
2. 用 DeepLabv3+ 刷 Cityscapes 到 75 mIoU，同时学会 TensorRT 加速（两周）  
3. 自己采集 1 张 GPU-内存受限板子，把 BiSeNetV2 压到 2 M 参数，30 fps 跑通（两周）  
4. 用 Swin-T + Mask2Former 做多任务，写论文级消融实验（三周）  
5. 把整套代码封装成 Docker + CI，交付给非算法同事一键训练/部署（一周）
